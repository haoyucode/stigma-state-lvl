{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7ee545-94dc-4313-a611-0f6fc38b51d4",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# Stigma against Opioid Use Disorder varies by Personal Use status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a0ab3-27a0-4d2c-9f3c-8bdd71b7a1c6",
   "metadata": {
    "tags": [
     "margin",
     "remove-input"
    ]
   },
   "source": [
    "```{margin} \n",
    "**To follow the full analysis, click through the hidden analysis code below**\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6df4a-6479-4274-aba2-34157f64eee6",
   "metadata": {
    "tags": [
     "hide-input",
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bootstrap\n",
    "import plotly_express as px\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55a4f9-ad51-47dd-be63-e77890aa430c",
   "metadata": {
    "tags": [
     "hide-input",
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "# import data and metadata (data dictionaries)\n",
    "os.chdir(\"P:/3645/Common/Protocol 2 Custom Survey/Analysis/Data File/\")\n",
    "\n",
    "df, meta = pyreadstat.read_sav('./3645_JCOIN_HEAL Initiative 2021_NORC_Jan2022_1.sav',apply_value_formats=True)\n",
    "\n",
    "## import data\n",
    "#df1 = pd.read_csv(\"jcoin_norc_omnibus_survey1_feb2020_072821-data.tsv\", sep='\\t')\n",
    "#df2 = pd.read_csv(\"jcoin_norc_omnibus_survey2_april2020_072821-data.tsv\", sep='\\t')\n",
    "#df3 = pd.read_csv(\"jcoin_norc_omnibus_survey3_june2020_072821-data.tsv\", sep='\\t')\n",
    "#df4 = pd.read_csv(\"jcoin_norc_omnibus_survey4_oct2020_072821-data.tsv\", sep='\\t')\n",
    "\n",
    "## import metadata (data dictionaries)\n",
    "#meta1 = pd.read_csv(\"jcoin_norc_omnibus_survey1_feb2020_072821-data_dictionary.tsv\", sep='\\t')\n",
    "#meta2 = pd.read_csv(\"jcoin_norc_omnibus_survey2_april2020_072821-data_dictionary.tsv\", sep='\\t')\n",
    "#meta3 = pd.read_csv(\"jcoin_norc_omnibus_survey3_june2020_072821-data_dictionary.tsv\", sep='\\t')\n",
    "#meta4 = pd.read_csv(\"jcoin_norc_omnibus_survey4_oct2020_072821-data_dictionary.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ecaa4e-433d-417d-a3a6-384081b42a1e",
   "metadata": {
    "tags": [
     "hide-input",
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow down the dataset to only variables that are collected at each of the time-points\n",
    "\n",
    "# standardize column names across datasets and metadatasets\n",
    "for df in [df]:\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "#for meta in [meta]:\n",
    "#    meta.name = meta.name.str.lower()  \n",
    "    \n",
    "## combine data from all surveys into one long dataset, keeping only vars available across all survey datasets (time-points)\n",
    "#all_df = pd.concat([df1, df2, df3, df4], keys=['feb2020','apr2020','jun2020','oct2020'], join=\"inner\").reset_index().rename(columns={\"level_0\": \"time-point\"})\n",
    "\n",
    "## combine metadata (data dictionaries) from all surveys into one long metadataset (data dictionary), keeping only vars available across all survey datasets\n",
    "## stacking these and sorting by variable name allow the data user to compare the variable description acrosss time-points to ensure they are consistent, or alert the data-user to inconsistencies in variable description/definition across time-points \n",
    "#all_meta = pd.concat([meta1, meta2, meta3, meta4], keys=['feb2020','apr2020','jun2020','oct2020'], join=\"inner\").reset_index().rename(columns={\"level_0\": \"time-point\"})\n",
    "#all_meta = all_meta[all_meta['name'].isin(all_df.columns)].sort_values(by = ['name']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-filing",
   "metadata": {
    "tags": [
     "hide-input",
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "# narrow down the dataset to only a few interesting (and relatively clean, straightforward variables) - check for missingness and impute to fill in missing\n",
    "\n",
    "# get subset of interesting variables\n",
    "#sub_df_1 = all_df[['time-point','weight','stigma_scale_score','age4','region4','personaluse_ever']]\n",
    "sub_df_1 = df[['p_over','weight1','weight2','stigma_scale_score','expanded_10item_stigma','state','age4','racethnicity','educ5','personaluse_ever','familyuse_ever','personalcrimjust_ever','familycrimjust_ever']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ee5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b25d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing I\n",
    "\n",
    "# check if missing values\n",
    "print(\"missing values: \")\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n",
    "# check if missing values\n",
    "print(\"missing values: \")\n",
    "print(sub_df_1.isna().sum())\n",
    "\n",
    "# get all var types\n",
    "#sub_df_1.info()\n",
    "print(\"var info: \")\n",
    "print(sub_df_1.info())\n",
    "\n",
    "# summary of numeric vars (weight and stigma_scale_score)\n",
    "print(\"summary of numeric vars: \")\n",
    "print(sub_df_1.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing II\n",
    "\n",
    "# summary of cat vars (p_over, state, age4, racethnicity, educ5, personaluse_ever, familyuse_ever, personalcrimjustice_ever)\n",
    "\n",
    "print(\"summary of cat var p_over: \")\n",
    "print(sub_df_1.p_over.value_counts(dropna=False))\n",
    "\n",
    "print(\"summary of cat var state: \")\n",
    "print(sub_df_1.state.value_counts(dropna=False))\n",
    "\n",
    "print(\"summary of cat var age4: \")\n",
    "print(sub_df_1.age4.value_counts(dropna=False))\n",
    "\n",
    "# summary of cat vars (age4, region9, personaluse_ever)\n",
    "print(\"summary of cat var racethnicity: \")\n",
    "print(sub_df_1.racethnicity.value_counts(dropna=False))\n",
    "\n",
    "print(\"summary of cat var educ5: \")\n",
    "print(sub_df_1.educ5.value_counts(dropna=False))\n",
    "\n",
    "# summary of cat vars (age4, region9, personaluse_ever)\n",
    "print(\"summary of cat var personaluse_ever: \")\n",
    "print(sub_df_1.personaluse_ever.value_counts(dropna=False))\n",
    "\n",
    "print(\"summary of cat var familyuse_ever: \")\n",
    "print(sub_df_1.familyuse_ever.value_counts(dropna=False))\n",
    "\n",
    "print(\"summary of cat var personalcrimjust_ever: \")\n",
    "print(sub_df_1.personalcrimjust_ever.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert familycrimjust_ever var from 1/0 to yes/no to be in line with other covar format\n",
    "\n",
    "#df.replace({0: 10, 1: 100},inplace=True)\n",
    "sub_df_1.familycrimjust_ever.replace({0:\"No\",1:\"Yes\"},inplace=True)\n",
    "print(sub_df_1.familycrimjust_ever.value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# familyuse_ever var was coded as yes/' no' (space before 'no') instead of yes/no; convert it to be yes/no to be in line with other covar format\n",
    "\n",
    "print(sub_df_1.familyuse_ever.value_counts(dropna=False))\n",
    "sub_df_1.familyuse_ever.replace({\" No\":\"No\"},inplace=True)\n",
    "print(sub_df_1.familyuse_ever.value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# personalcrimjust_ever was coded as Yes, ever arrested or incarcerated/No, never arrested or incarcerated; convert it to be yes/no to be in line with other covar format\n",
    "\n",
    "print(sub_df_1.personalcrimjust_ever.value_counts(dropna=False))\n",
    "#sub_df_1.personalcrimjust_ever.replace(regex = {r'^Yes.$': \"Yes\", r'^No.$': \"No\"}, inplace=True)\n",
    "sub_df_1.personalcrimjust_ever.replace({\"Yes, ever arrested or incarcerated\":\"Yes\", \"No, never arrested or incarcerated\":\"No\"},inplace=True)\n",
    "print(sub_df_1.personalcrimjust_ever.value_counts(dropna=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing stigma scale score vals with median per timepoint, impute missing personaluse_ever with mode, \"No\"\n",
    "\n",
    "# impute missing stigma scale score values as the median score by survey time-point\n",
    "#sub_df_1['stigma_scale_score'].fillna(sub_df_1.groupby('time-point')['stigma_scale_score'].transform('median'),inplace=True)\n",
    "sub_df_1['stigma_scale_score'].fillna(sub_df_1['stigma_scale_score'].median(),inplace=True)\n",
    "sub_df_1['expanded_10item_stigma'].fillna(sub_df_1['expanded_10item_stigma'].median(),inplace=True)\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n",
    "# replace missing values of personaluse_ever with mode value of 'No'\n",
    "sub_df_1.personaluse_ever.fillna('No',inplace=True)\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n",
    "sub_df_1.familyuse_ever.fillna('No',inplace=True)\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n",
    "sub_df_1.personalcrimjust_ever.fillna('No',inplace=True)\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n",
    "sub_df_1.familycrimjust_ever.fillna('No',inplace=True)\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abfb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bdbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all var types\n",
    "#sub_df_1.info()\n",
    "print(\"var info: \")\n",
    "print(sub_df_1.info())\n",
    "\n",
    "# confirm missing values corrected\n",
    "print(\"missing values: \")\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n",
    "# confirm missing values corrected\n",
    "print(\"missing values: \")\n",
    "print(sub_df_1.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pythonfix.com/code/us-states-abbrev.py/\n",
    "\n",
    "# state name to two letter code dictionary\n",
    "\n",
    "us_state_to_abbrev = {\n",
    "\"Alabama\": \"AL\",\n",
    "\"Alaska\": \"AK\",\n",
    "\"Arizona\": \"AZ\",\n",
    "\"Arkansas\": \"AR\",\n",
    "\"California\": \"CA\",\n",
    "\"Colorado\": \"CO\",\n",
    "\"Connecticut\": \"CT\",\n",
    "\"Delaware\": \"DE\",\n",
    "\"Florida\": \"FL\",\n",
    "\"Georgia\": \"GA\",\n",
    "\"Hawaii\": \"HI\",\n",
    "\"Idaho\": \"ID\",\n",
    "\"Illinois\": \"IL\",\n",
    "\"Indiana\": \"IN\",\n",
    "\"Iowa\": \"IA\",\n",
    "\"Kansas\": \"KS\",\n",
    "\"Kentucky\": \"KY\",\n",
    "\"Louisiana\": \"LA\",\n",
    "\"Maine\": \"ME\",\n",
    "\"Maryland\": \"MD\",\n",
    "\"Massachusetts\": \"MA\",\n",
    "\"Michigan\": \"MI\",\n",
    "\"Minnesota\": \"MN\",\n",
    "\"Mississippi\": \"MS\",\n",
    "\"Missouri\": \"MO\",\n",
    "\"Montana\": \"MT\",\n",
    "\"Nebraska\": \"NE\",\n",
    "\"Nevada\": \"NV\",\n",
    "\"New Hampshire\": \"NH\",\n",
    "\"New Jersey\": \"NJ\",\n",
    "\"New Mexico\": \"NM\",\n",
    "\"New York\": \"NY\",\n",
    "\"North Carolina\": \"NC\",\n",
    "\"North Dakota\": \"ND\",\n",
    "\"Ohio\": \"OH\",\n",
    "\"Oklahoma\": \"OK\",\n",
    "\"Oregon\": \"OR\",\n",
    "\"Pennsylvania\": \"PA\",\n",
    "\"Rhode Island\": \"RI\",\n",
    "\"South Carolina\": \"SC\",\n",
    "\"South Dakota\": \"SD\",\n",
    "\"Tennessee\": \"TN\",\n",
    "\"Texas\": \"TX\",\n",
    "\"Utah\": \"UT\",\n",
    "\"Vermont\": \"VT\",\n",
    "\"Virginia\": \"VA\",\n",
    "\"Washington\": \"WA\",\n",
    "\"West Virginia\": \"WV\",\n",
    "\"Wisconsin\": \"WI\",\n",
    "\"Wyoming\": \"WY\",\n",
    "\"District of Columbia\": \"DC\",\n",
    "\"American Samoa\": \"AS\",\n",
    "\"Guam\": \"GU\",\n",
    "\"Northern Mariana Islands\": \"MP\",\n",
    "\"Puerto Rico\": \"PR\",\n",
    "\"United States Minor Outlying Islands\": \"UM\",\n",
    "\"U.S. Virgin Islands\": \"VI\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add df column with state 2 letter code\n",
    "\n",
    "state_cd = sub_df_1.state.replace(us_state_to_abbrev)\n",
    "state_cd\n",
    "\n",
    "sub_df_1.insert(6,\"state_cd\",state_cd,True)\n",
    "sub_df_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-wells",
   "metadata": {
    "tags": [
     "hide-input",
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "# define function to calculate weighted mean of across the full population for survey vars - necessary to properly weight in order to retain the validity of nationally representative survey sampling strategy \n",
    "# https://stackoverflow.com/questions/32771520/how-to-use-a-weighted-mean-estimator-in-seaborn-factor-plot-incl-bootstrapping\n",
    "\n",
    "# sum of weights will not be equal to count of individuals when we look at sub-groups of the full population; \n",
    "# will have to formally calculate weighted average\n",
    "def weighted_mean(x, **kws):\n",
    "    val, weight = map(np.asarray, zip(*x))\n",
    "    return (val * weight).sum() / weight.sum()\n",
    "\n",
    "#sub_df_1[\"score_and_weight\"] = list(zip(sub_df_1.stigma_scale_score, sub_df_1.weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mwaskom/seaborn/issues/722\n",
    "\n",
    "def weighted_mean_2(x, **kws):\n",
    "    return np.sum(np.real(x) * np.imag(x)) / np.sum(np.imag(x))\n",
    "\n",
    "\n",
    "sub_df_1[\"score6_and_weight2\"] = [ v + w*1j for v,w in zip(sub_df_1.stigma_scale_score, sub_df_1.weight2)]\n",
    "sub_df_1[\"score10_and_weight2\"] = [ v + w*1j for v,w in zip(sub_df_1.expanded_10item_stigma, sub_df_1.weight2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavg(group, avg_name, weight_name):\n",
    "    \"\"\" http://stackoverflow.com/questions/10951341/pandas-dataframe-aggregate-function-using-multiple-columns\n",
    "    In rare instance, we may not have weights, so just return the mean. Customize this if your business case\n",
    "    should return otherwise.\n",
    "    \"\"\"\n",
    "    d = group[avg_name]\n",
    "    w = group[weight_name]\n",
    "    try:\n",
    "        return (d * w).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return d.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3fa5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# national estimate - method 1\n",
    "print(weighted_mean_2(sub_df_1[\"score6_and_weight2\"]))\n",
    "print(weighted_mean_2(sub_df_1[\"score10_and_weight2\"]))\n",
    "\n",
    "# national estimate - method 2\n",
    "print(wavg(sub_df_1,\"stigma_scale_score\",\"weight2\"))\n",
    "print(wavg(sub_df_1,\"expanded_10item_stigma\",\"weight2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state-level estimate - method 2 - all states\n",
    "\n",
    "print(sub_df_1.groupby(\"state\").apply(wavg,\"stigma_scale_score\",\"weight2\"))\n",
    "print(sub_df_1.groupby(\"state\").apply(wavg,\"expanded_10item_stigma\",\"weight2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state-level estimate - method 2 - only oversampled states\n",
    "\n",
    "# create new df with only state oversampled entries\n",
    "sub_df_2.state.cat.remove_unused_categories(inplace = True)\n",
    "\n",
    "# use new sub_df_2 to get state level estimates\n",
    "print(sub_df_2.groupby(\"state\").apply(wavg,\"stigma_scale_score\",\"weight2\"))\n",
    "print(sub_df_2.groupby(\"state\").apply(wavg,\"expanded_10item_stigma\",\"weight2\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc505b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# state-level estimate - method 2 - only oversampled states\n",
    "print(sub_df_1[sub_df_1.p_over == \"AS oversample\"].groupby(\"state\", observed = True).apply(wavg,\"stigma_scale_score\",\"weight2\"))\n",
    "print(sub_df_1[sub_df_1.p_over == \"AS oversample\"].groupby(\"state\", observed = True).apply(wavg,\"expanded_10item_stigma\",\"weight2\"))\n",
    "\n",
    "state_lvl_est_6 = sub_df_1[sub_df_1.p_over == \"AS oversample\"].groupby(\"state\", observed = True).apply(wavg,\"stigma_scale_score\",\"weight2\")\n",
    "state_lvl_est_10 = sub_df_1[sub_df_1.p_over == \"AS oversample\"].groupby(\"state\", observed = True).apply(wavg,\"expanded_10item_stigma\",\"weight2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_lvl_est_6 = sub_df_1[sub_df_1.p_over == \"AS oversample\"].groupby(\"state\", observed = True).apply(wavg,\"stigma_scale_score\",\"weight2\")\n",
    "\n",
    "state_lvl_est_6 = pd.DataFrame(state_lvl_est_6).reset_index().rename(columns={0:\"stigma_scale_score\"}).sort_values(by=[\"stigma_scale_score\"],ascending=False)\n",
    "state_lvl_est_6.head(10)\n",
    "\n",
    "# add df column with state 2 letter code\n",
    "\n",
    "\n",
    "\n",
    "def add_state_cd_to_df(df,state_name_col,insert_col,state_cd_col_name=\"state_cd\"):\n",
    "    # for df column with state full name, \n",
    "    # add df column with state 2 letter code\n",
    "    # at desired location\n",
    "\n",
    "    state_cd = df[state_name_col].replace(us_state_to_abbrev)\n",
    "    state_cd\n",
    "\n",
    "    # for now, leave in true to allow duplicates; should think about changing this though\n",
    "    df.insert(insert_col,state_cd_col_name,state_cd,True)\n",
    "    return df\n",
    "\n",
    "state_lvl_est_6 = add_state_cd_to_df(df=state_lvl_est_6, state_name_col=\"state\", insert_col=1)\n",
    "state_lvl_est_6.head(10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick map of state level estimates\n",
    "\n",
    "fig = px.choropleth(state_lvl_est_6, locations=\"state_cd\", locationmode=\"USA-states\",scope=\"usa\",color=\"stigma_scale_score\",color_continuous_scale=\"Viridis_r\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff86c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over a grouped df: http://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#:~:text=the%20passed%20key.-,Iterating%20through%20groups,-%23\n",
    "\n",
    "grouped_by_state = sub_df_2.groupby(\"state\")\n",
    "\n",
    "for name, group in grouped_by_state:\n",
    "    print(name)\n",
    "    print(group.weight2)\n",
    "\n",
    "# https://allendowney.github.io/ElementsOfDataScience/12_bootstrap.html#weighted-bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b13542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bootstrap\n",
    "\n",
    "# get a numpy array of stigma score values per state - need this to feed into scipy bootstrap function\n",
    "state_stigma_value_arrays = sub_df_2.stigma_scale_score.groupby(sub_df_2.state).apply(np.array).values\n",
    "print(state_stigma_value_arrays.ndim)\n",
    "print(state_stigma_value_arrays.shape)\n",
    "#print(state_stigma_value_arrays[5,])\n",
    "print(state_stigma_value_arrays[14,].ndim)\n",
    "print(state_stigma_value_arrays[14,].shape)\n",
    "\n",
    "# get a numpy array of corresponding weight2 per state - need this to feed into scipy bootstrap function\n",
    "state_stigma_value_arrays = sub_df_2.stigma_scale_score.groupby(sub_df_2.state).apply(np.array).values\n",
    "print(state_stigma_value_arrays.ndim)\n",
    "print(state_stigma_value_arrays.shape)\n",
    "#print(state_stigma_value_arrays[5,])\n",
    "print(state_stigma_value_arrays[14,].ndim)\n",
    "print(state_stigma_value_arrays[14,].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "from scipy.stats import norm\n",
    "dist = norm(loc=2, scale=4) \n",
    "data = (dist.rvs(size=(n_trials, 100), random_state=rng),)\n",
    "#res = bootstrap(data, np.std, axis=-1, confidence_level=0.9, n_resamples=1000, random_state=rng)\n",
    "#ci_l, ci_u = res.confidence_interval\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all var types\n",
    "#sub_df_1.info()\n",
    "print(\"var info: \")\n",
    "print(sub_df_1.info())\n",
    "\n",
    "sub_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad661c-cc91-4ce8-a928-a36e6fabdb18",
   "metadata": {
    "tags": [
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "ax = sns.histplot(x=\"stigma_scale_score\", data=sub_df_1, binwidth=0.5)\n",
    "\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f6fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(x=\"stigma_scale_score\", data=sub_df_1, binwidth=0.5, weights=\"weight2\")\n",
    "\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(x=\"stigma_scale_score\", data=sub_df_1, binwidth=0.2, weights=\"weight2\", hue=\"personaluse_ever\", alpha = 0.6, stat=\"percent\", common_norm=False, multiple=\"layer\")\n",
    "# setting common_norm == False gives percent within each group\n",
    "\n",
    "\n",
    "#for i in ax.containers:\n",
    "#    ax.bar_label(i,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(x=\"stigma_scale_score\", data=sub_df_1, binwidth=0.2, weights=\"weight2\", hue=\"familyuse_ever\", alpha = 0.6, stat=\"percent\", common_norm=False, multiple=\"layer\")\n",
    "# setting common_norm == False gives percent within each group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(ax=ax, x=\"state\", y=\"score6_and_weight2\", data=sub_df_1, estimator=weighted_mean_2, orient='v')\n",
    "#ax.set_ylim(2, 3.55) \n",
    "\n",
    "plt.draw()\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45,horizontalalignment='right')\n",
    "\n",
    "lower = [line.get_ydata().min() for line in ax.lines]\n",
    "upper = [line.get_ydata().max() for line in ax.lines]\n",
    "#means = ax.collections[0].get_offsets()[:, 1]\n",
    "#means = ax.containers\n",
    "#means = ax.containers[0].get_offsets()[:, 1]\n",
    "#means = ax.containers[0].get_datavalues()[:, 1]\n",
    "means = [rectangle.get_height().max() for rectangle in ax.patches]\n",
    "labels = [line.get_xdata() for line in ax.lines]\n",
    "#print(lower)\n",
    "#print(means)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/54868292/extract-mean-and-confidence-intervals-from-seaborn-regplot\n",
    "\n",
    "sns.barplot(x=\"state\", y=\"score6_and_weight2\", data=sub_df_1, estimator=weighted_mean_2, orient='v')\n",
    "ax = plt.gca()\n",
    "lower = [line.get_ydata().min() for line in ax.lines]\n",
    "upper = [line.get_ydata().max() for line in ax.lines]\n",
    "#means = ax.collections[0].get_offsets()[:, 1]\n",
    "means = ax.containers\n",
    "means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(ax=ax, x=\"time-point\", y=\"score_and_weight_2\", data=sub_df_1, estimator=weighted_mean_2, orient='v', hue = \"personaluse_ever\")\n",
    "#ax.set_ylim(2, 3.55) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49eb4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "fig.suptitle('Stigma Scale Score over time',fontsize='x-large',fontweight='bold')\n",
    "\n",
    "one = sns.lineplot(ax = ax1,\n",
    "             data = sub_df_1,\n",
    "             x = 'time-point',\n",
    "             y = 'score_and_weight_2',\n",
    "             #estimator = 'mean'\n",
    "             estimator = weighted_mean_2\n",
    "             #hue = 'personaluse_ever'\n",
    "            )\n",
    "\n",
    "plt.draw()\n",
    "one.set_xticks(one.get_xticks())\n",
    "one.set_xticklabels(one.get_xticklabels(),rotation=45,horizontalalignment='right')\n",
    "one.set_ylim(5, 1) # this is the average of 6 5-point likert scale questions; set the min/max accordingly\n",
    "#for tick in one.get_xticklabels():\n",
    "#    tick.set_rotation(45)\n",
    "    \n",
    "    \n",
    "ax1.set_title('Stigma Scale Score')\n",
    "ax1.set(ylabel='Stigma Scale Score')\n",
    "ax1.set(xlabel='')\n",
    "ax1.invert_yaxis()\n",
    "#ax1.set_xticks(ax1.get_xticks())  # just get and reset whatever you already have\n",
    "#one.set_xticklabels(one.get_xticklabels(),rotation=45,horizontalalignment='right')  # set the new/modified labels\n",
    "\n",
    "two = sns.lineplot(ax = ax2,\n",
    "             data = sub_df_1,\n",
    "             x = 'time-point',\n",
    "             y = 'score_and_weight_2',\n",
    "             hue = 'personaluse_ever',\n",
    "             #estimator = 'mean'\n",
    "             estimator = weighted_mean_2\n",
    "            )\n",
    "\n",
    "plt.draw()\n",
    "two.set_xticks(two.get_xticks())\n",
    "two.set_xticklabels(two.get_xticklabels(),rotation=45,horizontalalignment='right')\n",
    "two.set_ylim(5, 1) # this is the average of 6 5-point likert scale questions; set the min/max accordingly\n",
    "\n",
    "ax2.set_title('Stigma Scale Score\\nby Personal Use Status')\n",
    "ax2.set(xlabel='')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "legend2 = ax2.legend()\n",
    "#legend2.remove()\n",
    "\n",
    "#leg = ax2.legend(loc='center', bbox_to_anchor=(1.1, -0.25), shadow=False, ncol=2, frameon=False)\n",
    "\n",
    "plt.subplots_adjust(top=0.80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cabba9a-1be6-4169-b92f-d2e6aa28ba26",
   "metadata": {
    "tags": [
     "margin",
     "remove-input"
    ]
   },
   "source": [
    "```{margin} \n",
    "**To go to the data/study page on the HEAL Data Platform, follow this link:** my link\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9e924-0014-48c4-a7ad-f83be7a4c6bf",
   "metadata": {
    "tags": [
     "margin",
     "remove-input"
    ]
   },
   "source": [
    "```{margin} \n",
    "**To go to an interactive analytic cloud workspace with the analysis code and data loaded, follow this link:** my link\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2a3f4-dee6-495f-ab56-f3b8da047719",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Sodales ut eu sem integer vitae justo eget. Pellentesque dignissim enim sit amet venenatis urna cursus. Sed faucibus turpis in eu mi bibendum. Scelerisque felis imperdiet proin fermentum leo. Volutpat est velit egestas dui id ornare arcu. Quis lectus nulla at volutpat diam ut venenatis tellus. Tellus pellentesque eu tincidunt tortor aliquam nulla facilisi cras. Pellentesque adipiscing commodo elit at imperdiet dui. \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19603d20-965e-4480-9fa7-6cba2c2798dd",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "fig.suptitle('Stigma Scale Score over time',fontsize='x-large',fontweight='bold')\n",
    "\n",
    "one = sns.lineplot(ax = ax1,\n",
    "             data = sub_df_1,\n",
    "             x = 'time-point',\n",
    "             y = 'score_and_weight_2',\n",
    "             estimator = weighted_mean_2\n",
    "             #hue = 'personaluse_ever'\n",
    "            )\n",
    "\n",
    "plt.draw()\n",
    "one.set_xticks(one.get_xticks())\n",
    "one.set_xticklabels(one.get_xticklabels(),rotation=45,horizontalalignment='right')\n",
    "#for tick in one.get_xticklabels():\n",
    "#    tick.set_rotation(45)\n",
    "    \n",
    "    \n",
    "ax1.set_title('Stigma Scale Score')\n",
    "ax1.set(ylabel='Stigma Scale Score')\n",
    "ax1.set(xlabel='')\n",
    "ax1.invert_yaxis()\n",
    "#ax1.set_xticks(ax1.get_xticks())  # just get and reset whatever you already have\n",
    "#one.set_xticklabels(one.get_xticklabels(),rotation=45,horizontalalignment='right')  # set the new/modified labels\n",
    "\n",
    "two = sns.lineplot(ax = ax2,\n",
    "             data = sub_df_1,\n",
    "             x = 'time-point',\n",
    "             y = 'score_and_weight_2',\n",
    "             hue = 'personaluse_ever',\n",
    "             estimator = weighted_mean_2\n",
    "            )\n",
    "\n",
    "plt.draw()\n",
    "two.set_xticks(two.get_xticks())\n",
    "two.set_xticklabels(two.get_xticklabels(),rotation=45,horizontalalignment='right')\n",
    "\n",
    "ax2.set_title('Stigma Scale Score\\nby Personal Use Status')\n",
    "ax2.set(xlabel='')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "legend2 = ax2.legend()\n",
    "#legend2.remove()\n",
    "\n",
    "#leg = ax2.legend(loc='center', bbox_to_anchor=(1.1, -0.25), shadow=False, ncol=2, frameon=False)\n",
    "\n",
    "plt.subplots_adjust(top=0.80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89360e25-0923-451a-bd3a-d6e4eb1e6f6d",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Sodales ut eu sem integer vitae justo eget. Pellentesque dignissim enim sit amet venenatis urna cursus. Sed faucibus turpis in eu mi bibendum. Scelerisque felis imperdiet proin fermentum leo. Volutpat est velit egestas dui id ornare arcu. Quis lectus nulla at volutpat diam ut venenatis tellus. Tellus pellentesque eu tincidunt tortor aliquam nulla facilisi cras. Pellentesque adipiscing commodo elit at imperdiet dui. \n",
    "<br><br>\n",
    "In hac habitasse platea dictumst quisque sagittis purus. Libero volutpat sed cras ornare. Sit amet consectetur adipiscing elit pellentesque habitant morbi tristique senectus. Auctor augue mauris augue neque gravida in fermentum et. Amet mattis vulputate enim nulla aliquet porttitor. Proin sed libero enim sed faucibus turpis in eu. Morbi tristique senectus et netus et malesuada. Feugiat sed lectus vestibulum mattis ullamcorper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9be514-eb8f-4aa2-836f-854bd0ce1301",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "**Data Citation** \n",
    "<br>\n",
    "Harold Pollack, Johnathon Schneider, Bruce Taylor. JCOIN 026: Brief Stigma Survey. Chicago, IL: Center for Translational Data Science HEAL Data Platform (distributor) via Center for Translational Data Science JCOIN Data Commons (repository & distributor), 2022-04-08. (HEAL Data Platform branded doi goes here)\n",
    "<br>\n",
    "**Brief Article Citation** \n",
    "<br>\n",
    "What format should this be? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stigma_survey_p1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
