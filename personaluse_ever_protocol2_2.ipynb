{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7ee545-94dc-4313-a611-0f6fc38b51d4",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# Stigma against Opioid Use Disorder varies by Personal Use status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a0ab3-27a0-4d2c-9f3c-8bdd71b7a1c6",
   "metadata": {
    "tags": [
     "margin",
     "remove-input"
    ]
   },
   "source": [
    "```{margin} \n",
    "**To follow the full analysis, click through the hidden analysis code below**\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6df4a-6479-4274-aba2-34157f64eee6",
   "metadata": {
    "tags": [
     "hide-input",
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bootstrap\n",
    "import plotly.express as px\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ded81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "STATE_ABBREVIATIONS = \"state_abbrev_mappings.json\"\n",
    "DATA_FILE = (\n",
    "    \"P:/3645/Common/Protocol 2 Custom Survey/\"\n",
    "    \"Analysis/Data File/\"\n",
    "    \"3645_JCOIN_HEAL Initiative 2021_NORC_Jan2022_1.sav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55a4f9-ad51-47dd-be63-e77890aa430c",
   "metadata": {
    "tags": [
     "hide-input",
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "# import data and metadata (data dictionaries)\n",
    "df, meta = pyreadstat.read_sav(DATA_FILE,apply_value_formats=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ecaa4e-433d-417d-a3a6-384081b42a1e",
   "metadata": {
    "tags": [
     "hide-input",
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow down the dataset to only variables that are collected at each of the time-points\n",
    "\n",
    "# standardize column names across datasets and metadatasets\n",
    "for df in [df]:\n",
    "    df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a607ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_of_interest = ['p_over','weight1','weight2','stigma_scale_score','expanded_10item_stigma','state','age4','racethnicity','educ5','personaluse_ever','familyuse_ever','personalcrimjust_ever','familycrimjust_ever']\n",
    "categorical_vars = ['p_over','state','age4','racethnicity','educ5',\n",
    "    'personaluse_ever','familyuse_ever',\n",
    "    'personalcrimjust_ever','familycrimjust_ever']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-filing",
   "metadata": {
    "tags": [
     "hide-input",
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "# narrow down the dataset to only a few interesting (and relatively clean, straightforward variables) - check for missingness and impute to fill in missing\n",
    "sub_df_1 = df[vars_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ee5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c8c77b6",
   "metadata": {},
   "source": [
    "## Missing values and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b25d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing I\n",
    "\n",
    "# check if missing values\n",
    "print(\"missing values (.isnull): \")\n",
    "print(sub_df_1.isnull().sum())\n",
    "print()\n",
    "# check if missing values\n",
    "print(\"missing values (.isna): \")\n",
    "print(sub_df_1.isna().sum())\n",
    "print()\n",
    "# get all var types\n",
    "#sub_df_1.info()\n",
    "print(\"var info (.info): \")\n",
    "print(sub_df_1.info())\n",
    "print()\n",
    "# summary of numeric vars (weight and stigma_scale_score)\n",
    "print(\"summary of numeric vars (.describe): \")\n",
    "print(sub_df_1.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing II\n",
    "# summary of cat vars\n",
    "for v in categorical_vars:\n",
    "    print(f\"summary of cat var {v}: \")\n",
    "    print(sub_df_1[v].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert familycrimjust_ever var from 1/0 to yes/no to be in line with other covar format\n",
    "\n",
    "#df.replace({0: 10, 1: 100},inplace=True)\n",
    "sub_df_1.familycrimjust_ever.replace({0:\"No\",1:\"Yes\"},inplace=True)\n",
    "print(sub_df_1.familycrimjust_ever.value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# familyuse_ever var was coded as yes/' no' (space before 'no') instead of yes/no; convert it to be yes/no to be in line with other covar format\n",
    "\n",
    "print(sub_df_1.familyuse_ever.value_counts(dropna=False))\n",
    "sub_df_1.familyuse_ever.replace({\" No\":\"No\"},inplace=True)\n",
    "print(sub_df_1.familyuse_ever.value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# personalcrimjust_ever was coded as Yes, ever arrested or incarcerated/No, never arrested or incarcerated; convert it to be yes/no to be in line with other covar format\n",
    "\n",
    "print(sub_df_1.personalcrimjust_ever.value_counts(dropna=False))\n",
    "#sub_df_1.personalcrimjust_ever.replace(regex = {r'^Yes.$': \"Yes\", r'^No.$': \"No\"}, inplace=True)\n",
    "sub_df_1.personalcrimjust_ever.replace({\"Yes, ever arrested or incarcerated\":\"Yes\", \"No, never arrested or incarcerated\":\"No\"},inplace=True)\n",
    "print(sub_df_1.personalcrimjust_ever.value_counts(dropna=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing stigma scale score vals with median per timepoint, impute missing personaluse_ever with mode, \"No\"\n",
    "\n",
    "# impute missing stigma scale score values as the median score by survey time-point\n",
    "#sub_df_1['stigma_scale_score'].fillna(sub_df_1.groupby('time-point')['stigma_scale_score'].transform('median'),inplace=True)\n",
    "sub_df_1['stigma_scale_score'].fillna(sub_df_1['stigma_scale_score'].median(),inplace=True)\n",
    "sub_df_1['expanded_10item_stigma'].fillna(sub_df_1['expanded_10item_stigma'].median(),inplace=True)\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n",
    "# replace missing values of personaluse_ever with mode value of 'No'\n",
    "sub_df_1.personaluse_ever.fillna('No',inplace=True)\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n",
    "sub_df_1.familyuse_ever.fillna('No',inplace=True)\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n",
    "sub_df_1.personalcrimjust_ever.fillna('No',inplace=True)\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n",
    "sub_df_1.familycrimjust_ever.fillna('No',inplace=True)\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abfb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bdbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all var types\n",
    "#sub_df_1.info()\n",
    "print(\"var info: \")\n",
    "print(sub_df_1.info())\n",
    "\n",
    "# confirm missing values corrected\n",
    "print(\"missing values: \")\n",
    "print(sub_df_1.isnull().sum())\n",
    "\n",
    "# confirm missing values corrected\n",
    "print(\"missing values: \")\n",
    "print(sub_df_1.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b704415",
   "metadata": {},
   "source": [
    "## State level weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pythonfix.com/code/us-states-abbrev.py/\n",
    "\n",
    "# state name to two letter code dictionary\n",
    "us_state_to_abbrev = json.loads(Path(STATE_ABBREVIATIONS).read_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add df column with state 2 letter code\n",
    "\n",
    "state_cd = sub_df_1.state.replace(us_state_to_abbrev)\n",
    "state_cd\n",
    "\n",
    "sub_df_1.insert(6,\"state_cd\",state_cd,True)\n",
    "sub_df_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-wells",
   "metadata": {
    "tags": [
     "hide-input",
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "# define function to calculate weighted mean of across the full population for survey vars - necessary to properly weight in order to retain the validity of nationally representative survey sampling strategy \n",
    "# https://stackoverflow.com/questions/32771520/how-to-use-a-weighted-mean-estimator-in-seaborn-factor-plot-incl-bootstrapping\n",
    "\n",
    "# sum of weights will not be equal to count of individuals when we look at sub-groups of the full population; \n",
    "# will have to formally calculate weighted average\n",
    "def weighted_mean(x, **kws):\n",
    "    val, weight = map(np.asarray, zip(*x))\n",
    "    return (val * weight).sum() / weight.sum()\n",
    "\n",
    "#sub_df_1[\"score_and_weight\"] = list(zip(sub_df_1.stigma_scale_score, sub_df_1.weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mwaskom/seaborn/issues/722\n",
    "\n",
    "def weighted_mean_2(x, **kws):\n",
    "    return np.sum(np.real(x) * np.imag(x)) / np.sum(np.imag(x))\n",
    "\n",
    "\n",
    "sub_df_1[\"score6_and_weight2\"] = [ v + w*1j for v,w in zip(sub_df_1.stigma_scale_score, sub_df_1.weight2)]\n",
    "sub_df_1[\"score10_and_weight2\"] = [ v + w*1j for v,w in zip(sub_df_1.expanded_10item_stigma, sub_df_1.weight2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavg(group, avg_name, weight_name):\n",
    "    \"\"\" http://stackoverflow.com/questions/10951341/pandas-dataframe-aggregate-function-using-multiple-columns\n",
    "    In rare instance, we may not have weights, so just return the mean. Customize this if your business case\n",
    "    should return otherwise.\n",
    "    \"\"\"\n",
    "    d = group[avg_name]\n",
    "    w = group[weight_name]\n",
    "    try:\n",
    "        return (d * w).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return d.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3fa5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# national estimate - method 1\n",
    "print(weighted_mean_2(sub_df_1[\"score6_and_weight2\"]))\n",
    "print(weighted_mean_2(sub_df_1[\"score10_and_weight2\"]))\n",
    "\n",
    "# national estimate - method 2\n",
    "print(wavg(sub_df_1,\"stigma_scale_score\",\"weight2\"))\n",
    "print(wavg(sub_df_1,\"expanded_10item_stigma\",\"weight2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state-level estimate - method 2 - all states\n",
    "\n",
    "print(sub_df_1.groupby(\"state\").apply(wavg,\"stigma_scale_score\",\"weight2\"))\n",
    "print(sub_df_1.groupby(\"state\").apply(wavg,\"expanded_10item_stigma\",\"weight2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state-level estimate - method 2 - only oversampled states\n",
    "\n",
    "# create new df with only state oversampled entries\n",
    "sub_df_2 = sub_df_1.copy()\n",
    "sub_df_2.state.cat.remove_unused_categories(inplace = True)\n",
    "\n",
    "# use new sub_df_2 to get state level estimates\n",
    "print(sub_df_2.groupby(\"state\").apply(wavg,\"stigma_scale_score\",\"weight2\"))\n",
    "print(sub_df_2.groupby(\"state\").apply(wavg,\"expanded_10item_stigma\",\"weight2\"))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c792d03",
   "metadata": {},
   "source": [
    "Get count of obs in oversampled states but not part of oversampled population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56cc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\ is line continuation\n",
    "oversampled_states = sub_df_1\\\n",
    "    [sub_df_1.p_over == \"AS oversample\"]\\\n",
    "    ['state']\\\n",
    "    .cat.remove_unused_categories()\\\n",
    "    .cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aca3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample pop\n",
    "oversampled_pop_counts = (\n",
    "    sub_df_1\n",
    "    [lambda df:(df.p_over == \"AS oversample\") & (df['state'].isin(oversampled_states))]\n",
    "    ['state']\n",
    "    .value_counts(dropna=False)\n",
    ")\n",
    "print(\"TOTAL N:\")\n",
    "print(oversampled_pop_counts.sum())\n",
    "print(\"N per State:\")\n",
    "print(\n",
    "    oversampled_pop_counts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampled states but not oversampled pop\n",
    "print(\n",
    "    sub_df_1\n",
    "    [lambda df:(df.p_over != \"AS oversample\") & (df['state'].isin(oversampled_states))]\n",
    "    ['state']\n",
    "    .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc505b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state-level estimate - method 2 - only oversampled states \n",
    "isin_oversampled_state = sub_df_1[\"state\"].isin(oversampled_states)\n",
    "print(sub_df_1.loc[isin_oversampled_state,:].groupby(\"state\", observed = True).apply(wavg,\"stigma_scale_score\",\"weight2\"))\n",
    "print(sub_df_1.loc[isin_oversampled_state,:].groupby(\"state\", observed = True).apply(wavg,\"expanded_10item_stigma\",\"weight2\"))\n",
    "\n",
    "state_lvl_est_6 = sub_df_1.loc[isin_oversampled_state,:].groupby(\"state\", observed = True).apply(wavg,\"stigma_scale_score\",\"weight2\")\n",
    "state_lvl_est_10 = sub_df_1.loc[isin_oversampled_state,:].groupby(\"state\", observed = True).apply(wavg,\"expanded_10item_stigma\",\"weight2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_lvl_est_6 = sub_df_1[sub_df_1.p_over == \"AS oversample\"].groupby(\"state\", observed = True).apply(wavg,\"stigma_scale_score\",\"weight2\")\n",
    "\n",
    "state_lvl_est_6 = pd.DataFrame(state_lvl_est_6).reset_index().rename(columns={0:\"stigma_scale_score\"}).sort_values(by=[\"stigma_scale_score\"],ascending=False)\n",
    "state_lvl_est_6.head(10)\n",
    "\n",
    "# add df column with state 2 letter code\n",
    "def add_state_cd_to_df(df,state_name_col,insert_col,state_cd_col_name=\"state_cd\"):\n",
    "    # for df column with state full name, \n",
    "    # add df column with state 2 letter code\n",
    "    # at desired location\n",
    "\n",
    "    state_cd = df[state_name_col].replace(us_state_to_abbrev)\n",
    "    state_cd\n",
    "\n",
    "    # for now, leave in true to allow duplicates; should think about changing this though\n",
    "    df.insert(insert_col,state_cd_col_name,state_cd,True)\n",
    "    return df\n",
    "\n",
    "state_lvl_est_6 = add_state_cd_to_df(df=state_lvl_est_6, state_name_col=\"state\", insert_col=1)\n",
    "state_lvl_est_6.head(10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick map of state level estimates\n",
    "\n",
    "fig = px.choropleth(state_lvl_est_6,\n",
    "    locations=\"state_cd\",\n",
    "    locationmode=\"USA-states\",\n",
    "    scope=\"usa\",\n",
    "    color=\"stigma_scale_score\",\n",
    "    color_continuous_scale=\"Viridis_r\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff86c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over a grouped df: http://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#:~:text=the%20passed%20key.-,Iterating%20through%20groups,-%23\n",
    "\n",
    "grouped_by_state = sub_df_2.groupby(\"state\")\n",
    "\n",
    "for name, group in grouped_by_state:\n",
    "    print(name)\n",
    "    print(group.weight2)\n",
    "\n",
    "# https://allendowney.github.io/ElementsOfDataScience/12_bootstrap.html#weighted-bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b13542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bootstrap\n",
    "\n",
    "# get a numpy array of stigma score values per state - need this to feed into scipy bootstrap function\n",
    "state_stigma_value_arrays = sub_df_2.stigma_scale_score.groupby(sub_df_2.state).apply(np.array).values\n",
    "print(state_stigma_value_arrays.ndim)\n",
    "print(state_stigma_value_arrays.shape)\n",
    "#print(state_stigma_value_arrays[5,])\n",
    "print(state_stigma_value_arrays[14,].ndim)\n",
    "print(state_stigma_value_arrays[14,].shape)\n",
    "\n",
    "# get a numpy array of corresponding weight2 per state - need this to feed into scipy bootstrap function\n",
    "state_stigma_value_arrays = sub_df_2.stigma_scale_score.groupby(sub_df_2.state).apply(np.array).values\n",
    "print(state_stigma_value_arrays.ndim)\n",
    "print(state_stigma_value_arrays.shape)\n",
    "#print(state_stigma_value_arrays[5,])\n",
    "print(state_stigma_value_arrays[14,].ndim)\n",
    "print(state_stigma_value_arrays[14,].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html\n",
    "n_trials = 5000\n",
    "rng = np.random.default_rng()\n",
    "from scipy.stats import norm\n",
    "dist = norm(loc=2, scale=4) \n",
    "data = (dist.rvs(size=(n_trials, 100), random_state=rng),)\n",
    "#res = bootstrap(data, np.std, axis=-1, confidence_level=0.9, n_resamples=1000, random_state=rng)\n",
    "#ci_l, ci_u = res.confidence_interval\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all var types\n",
    "#sub_df_1.info()\n",
    "print(\"var info: \")\n",
    "print(sub_df_1.info())\n",
    "\n",
    "sub_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad661c-cc91-4ce8-a928-a36e6fabdb18",
   "metadata": {
    "tags": [
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "ax = sns.histplot(x=\"stigma_scale_score\", data=sub_df_1, binwidth=0.5)\n",
    "\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f6fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(x=\"stigma_scale_score\", data=sub_df_1, binwidth=0.5, weights=\"weight2\")\n",
    "\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(x=\"stigma_scale_score\", data=sub_df_1, binwidth=0.2, weights=\"weight2\", hue=\"personaluse_ever\", alpha = 0.6, stat=\"percent\", common_norm=False, multiple=\"layer\")\n",
    "# setting common_norm == False gives percent within each group\n",
    "\n",
    "\n",
    "#for i in ax.containers:\n",
    "#    ax.bar_label(i,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(x=\"stigma_scale_score\", data=sub_df_1, binwidth=0.2, weights=\"weight2\", hue=\"familyuse_ever\", alpha = 0.6, stat=\"percent\", common_norm=False, multiple=\"layer\")\n",
    "# setting common_norm == False gives percent within each group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(ax=ax, x=\"state\", y=\"score6_and_weight2\", data=sub_df_1, estimator=weighted_mean_2, orient='v')\n",
    "#ax.set_ylim(2, 3.55) \n",
    "\n",
    "plt.draw()\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45,horizontalalignment='right')\n",
    "\n",
    "lower = [line.get_ydata().min() for line in ax.lines]\n",
    "upper = [line.get_ydata().max() for line in ax.lines]\n",
    "#means = ax.collections[0].get_offsets()[:, 1]\n",
    "#means = ax.containers\n",
    "#means = ax.containers[0].get_offsets()[:, 1]\n",
    "#means = ax.containers[0].get_datavalues()[:, 1]\n",
    "means = [rectangle.get_height().max() for rectangle in ax.patches]\n",
    "labels = [line.get_xdata() for line in ax.lines]\n",
    "#print(lower)\n",
    "#print(means)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d353cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jcoin_json = json.loads(Path(\"jcoin_states.json\").read_text())\n",
    "jcoin_df = pd.DataFrame(jcoin_json)\\\n",
    "    .assign(hub_types=lambda df:df[\"hub\"]+\"(\"+df[\"type\"]+\")\")\\\n",
    "    .groupby('states')\\\n",
    "    .agg(\n",
    "        {\"hub_types\":lambda s:\",\".join(s),\n",
    "        \"hub\":\"count\"}\n",
    "        )\\\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd8a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_jcoin_state = sub_df_1[\"state_cd\"].isin(jcoin_df.index.values)\n",
    "sub_df_1[\"is_jcoin_state\"] = np.where(\n",
    "    sub_df_1[\"state_cd\"].isin(jcoin_df[\"states\"]),\n",
    "        \"Yes\",\"No\" \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14479a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(\n",
    "    ax=ax, x=\"state\",\n",
    "     y=\"score6_and_weight2\", \n",
    "     data=sub_df_1.sort_values('is_jcoin_state'), \n",
    "     estimator=weighted_mean_2,\n",
    "     orient='v',\n",
    "     hue=\"is_jcoin_state\")\n",
    "#ax.set_ylim(2, 3.55) \n",
    "\n",
    "plt.draw()\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45,horizontalalignment='right')\n",
    "\n",
    "lower = [line.get_ydata().min() for line in ax.lines]\n",
    "upper = [line.get_ydata().max() for line in ax.lines]\n",
    "#means = ax.collections[0].get_offsets()[:, 1]\n",
    "#means = ax.containers\n",
    "#means = ax.containers[0].get_offsets()[:, 1]\n",
    "#means = ax.containers[0].get_datavalues()[:, 1]\n",
    "means = [rectangle.get_height().max() for rectangle in ax.patches]\n",
    "labels = [line.get_xdata() for line in ax.lines]\n",
    "#print(lower)\n",
    "#print(means)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_1[\"score6_and_weight2\"] = sub_df_1[\"score6_and_weight2\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.violinplot(\n",
    "    ax=ax, x=\"is_jcoin_state\",\n",
    "     y=\"score6_and_weight2\", \n",
    "     data=sub_df_1, \n",
    "     estimator=weighted_mean_2)\n",
    "#ax.set_ylim(2, 3.55) \n",
    "\n",
    "plt.draw()\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45,horizontalalignment='right')\n",
    "\n",
    "lower = [line.get_ydata().min() for line in ax.lines]\n",
    "upper = [line.get_ydata().max() for line in ax.lines]\n",
    "#means = ax.collections[0].get_offsets()[:, 1]\n",
    "#means = ax.containers\n",
    "#means = ax.containers[0].get_offsets()[:, 1]\n",
    "#means = ax.containers[0].get_datavalues()[:, 1]\n",
    "means = [rectangle.get_height().max() for rectangle in ax.patches]\n",
    "labels = [line.get_xdata() for line in ax.lines]\n",
    "#print(lower)\n",
    "#print(means)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cabba9a-1be6-4169-b92f-d2e6aa28ba26",
   "metadata": {
    "tags": [
     "margin",
     "remove-input"
    ]
   },
   "source": [
    "```{margin} \n",
    "**To go to the data/study page on the HEAL Data Platform, follow this link:** my link\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9e924-0014-48c4-a7ad-f83be7a4c6bf",
   "metadata": {
    "tags": [
     "margin",
     "remove-input"
    ]
   },
   "source": [
    "```{margin} \n",
    "**To go to an interactive analytic cloud workspace with the analysis code and data loaded, follow this link:** my link\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2a3f4-dee6-495f-ab56-f3b8da047719",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Sodales ut eu sem integer vitae justo eget. Pellentesque dignissim enim sit amet venenatis urna cursus. Sed faucibus turpis in eu mi bibendum. Scelerisque felis imperdiet proin fermentum leo. Volutpat est velit egestas dui id ornare arcu. Quis lectus nulla at volutpat diam ut venenatis tellus. Tellus pellentesque eu tincidunt tortor aliquam nulla facilisi cras. Pellentesque adipiscing commodo elit at imperdiet dui. \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89360e25-0923-451a-bd3a-d6e4eb1e6f6d",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Sodales ut eu sem integer vitae justo eget. Pellentesque dignissim enim sit amet venenatis urna cursus. Sed faucibus turpis in eu mi bibendum. Scelerisque felis imperdiet proin fermentum leo. Volutpat est velit egestas dui id ornare arcu. Quis lectus nulla at volutpat diam ut venenatis tellus. Tellus pellentesque eu tincidunt tortor aliquam nulla facilisi cras. Pellentesque adipiscing commodo elit at imperdiet dui. \n",
    "<br><br>\n",
    "In hac habitasse platea dictumst quisque sagittis purus. Libero volutpat sed cras ornare. Sit amet consectetur adipiscing elit pellentesque habitant morbi tristique senectus. Auctor augue mauris augue neque gravida in fermentum et. Amet mattis vulputate enim nulla aliquet porttitor. Proin sed libero enim sed faucibus turpis in eu. Morbi tristique senectus et netus et malesuada. Feugiat sed lectus vestibulum mattis ullamcorper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9be514-eb8f-4aa2-836f-854bd0ce1301",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "**Data Citation** \n",
    "<br>\n",
    "Harold Pollack, Johnathon Schneider, Bruce Taylor. JCOIN 026: Brief Stigma Survey. Chicago, IL: Center for Translational Data Science HEAL Data Platform (distributor) via Center for Translational Data Science JCOIN Data Commons (repository & distributor), 2022-04-08. (HEAL Data Platform branded doi goes here)\n",
    "<br>\n",
    "**Brief Article Citation** \n",
    "<br>\n",
    "What format should this be? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stigma_survey_p1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
