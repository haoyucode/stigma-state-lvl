{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "process data and save in data/processed folder\n",
    "TODO: refactor code in the data module (right now I just copy/pasted the relevant code from the legacy notebook in `data`)\n",
    "\n",
    " \"\"\"\n",
    "\n",
    "\"\"\" \n",
    "Creates analytic dataset for analyses\n",
    "TODO: create schema with additional metadata (ie descriptions for derived variables and scale groupings) for this dataset\n",
    "\n",
    "\"\"\"\n",
    "# %% [markdown]\n",
    "# # Stigma against Opioid Use Disorder varies by Personal Use status\n",
    "\n",
    "# %% [markdown]\n",
    "# ```{margin}\n",
    "# **To follow the full analysis, click through the hidden analysis code below**\n",
    "# ```\n",
    "\n",
    "\n",
    "# %%\n",
    "# import packages\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import frictionless\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Data cleaning/pre-processing\n",
    "# %%\n",
    "package = frictionless.Package.from_descriptor(\"datapackage.json\")\n",
    "\n",
    "# %%\n",
    "# import data and metadata (data dictionaries)\n",
    "datapath = package.get_resource(\"wave1-with-states\").path\n",
    "df, meta = pyreadstat.read_sav(datapath, apply_value_formats=True)\n",
    "# %%\n",
    "\n",
    "# lower-case column names\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# %%\n",
    "vars_of_interest = [\n",
    "    \"caseid\",\n",
    "    \"p_over\",\n",
    "    \"weight1\",\n",
    "    \"weight2\",\n",
    "    \"stigma_scale_score\",\n",
    "    \"expanded_10item_stigma\",\n",
    "    \"state\",\n",
    "    \"age4\",\n",
    "    \"racethnicity\",\n",
    "    \"educ5\",\n",
    "    \"personaluse_ever\",\n",
    "    \"familyuse_ever\",\n",
    "    \"personalcrimjust_ever\",\n",
    "    \"familycrimjust_ever\",\n",
    "]\n",
    "categorical_vars = [\n",
    "    \"p_over\",\n",
    "    \"state\",\n",
    "    \"age4\",\n",
    "    \"racethnicity\",\n",
    "    \"educ5\",\n",
    "    \"personaluse_ever\",\n",
    "    \"familyuse_ever\",\n",
    "    \"personalcrimjust_ever\",\n",
    "    \"familycrimjust_ever\",\n",
    "]\n",
    "\n",
    "\n",
    "# %%\n",
    "# to enable more granular analysis of the stigma scale score(s) - e.g. parsing impact of current versus past OUD on stigma - bring in the individual ss questions\n",
    "\n",
    "# ss_a_historywork - agree means low stigma/high val means low stigma\n",
    "# ss_b_historymarry - agree means low stigma/high val means low stigma\n",
    "# ss_c_currentwork - agree means low stigma/high val means low stigma\n",
    "# ss_d_currentmarry - agree means low stigma/high val means low stigma\n",
    "\n",
    "# -------- use the reverse coded for history and current work and marry vars - these ones are the only ss questions where agree means low stigma, using the reverse coded version brings them in line with the others for easier analysis\n",
    "\n",
    "# ss_a_historywork_rev - already converted to numeric/high val means high stigma\n",
    "# ss_b_historymarry_rev - already converted to numeric/high val means high stigma\n",
    "# ss_c_currentwork_rev - already converted to numeric/high val means high stigma\n",
    "# ss_d_currentmarry_rev - already converted to numeric/high val means high stigma\n",
    "\n",
    "# ss_e_dangerous - agree means high stigma/high val means high stigma\n",
    "# ss_f_ trust - agree means high stigma/high val means high stigma\n",
    "# ss_history_steal - agree means high stigma/high val means high stigma\n",
    "# ss_historyhighrisk - agree means high stigma/high val means high stigma\n",
    "# ss_currentsteal - agree means high stigma/high val means high stigma\n",
    "# ss_currenthighrisk - agree means high stigma/high val means high stigma\n",
    "\n",
    "ss_6_past = [\"ss_a_historywork_rev\", \"ss_b_historymarry_rev\"]\n",
    "ss_6_current = [\n",
    "    \"ss_c_currentwork_rev\",\n",
    "    \"ss_d_currentmarry_rev\",\n",
    "    \"ss_e_dangerous\",\n",
    "    \"ss_f_trust\",\n",
    "]\n",
    "\n",
    "ss_6_full = ss_6_past + ss_6_current\n",
    "\n",
    "ss_10_past = [\"ss_historysteal\", \"ss_historyhighrisk\"]\n",
    "ss_10_current = [\"ss_currentsteal\", \"ss_currenthighrisk\"]\n",
    "\n",
    "ss_10_full = ss_6_full + ss_10_past + ss_10_current\n",
    "\n",
    "ss_past = ss_6_past + ss_10_past\n",
    "ss_current = ss_6_current + ss_10_current\n",
    "\n",
    "\n",
    "# %%\n",
    "# to enable parsing of stigma by political affiliation, views on race/ethnicity, and experience of racial/ethnic discrimination bring in variables assessing those items\n",
    "\n",
    "# political = ['pid1','pida','pidb','pidi','partyid7','partyid5']\n",
    "political = [\"partyid5\"]\n",
    "\n",
    "race = [\"race_whiteadvantage\", \"race_rich\"]\n",
    "\n",
    "# race_whiteadvanctage: [White people in the U.S. have certain advantages because of the color of their skin.] Do you disagree or agree with the following statements?\n",
    "# agree corresponds with recognition of white advantage; high vals = recognition of white advantage\n",
    "# reverse code this from likert vars so that high vals will now indicate lack of recognition of white advantage\n",
    "\n",
    "# race_rich: [Everyone who works hard, no matter what race they are, has an equal chance to become rich.] Do you disagree or agree with the following statements?\n",
    "# agree corresponds with lack of recognition of white advantage; high vals = lack of recognition of white advantage\n",
    "# code this along with the likert vars where high vals = high stigma; in this case high vals = lack of recognition of white advantage\n",
    "\n",
    "# discrimination_experience = ['times_atschool', 'times_hired', 'times_atwork', 'times_housing', 'times_medcare', 'times_restaurant', 'times_credit', 'times_street', 'times_police']\n",
    "\n",
    "# possible approach:\n",
    "# add count of dicrimination experiences (times) across categories\n",
    "# higher numbers mean more discrimination experience\n",
    "\n",
    "\n",
    "# %%\n",
    "likert_replace_vars = [\n",
    "    \"ss_e_dangerous\",\n",
    "    \"ss_f_trust\",\n",
    "    \"ss_historysteal\",\n",
    "    \"ss_historyhighrisk\",\n",
    "    \"ss_currentsteal\",\n",
    "    \"ss_currenthighrisk\",\n",
    "    \"race_rich\",\n",
    "]\n",
    "likert_reverse_replace_vars = [\"race_whiteadvantage\"]\n",
    "\n",
    "# %%\n",
    "\n",
    "# additional_vars_of_interest = ss_10_full + political + race + discrimination_experience\n",
    "additional_vars_of_interest = [\"caseid\"] + ss_10_full + political + race\n",
    "all_vars_of_interest = vars_of_interest + additional_vars_of_interest\n",
    "\n",
    "\n",
    "# %%\n",
    "# narrow down the dataset to only a few interesting (and relatively clean, straightforward variables) - check for missingness and impute to fill in missing\n",
    "sub_df_1 = df[vars_of_interest]\n",
    "sub_df_2 = df[additional_vars_of_interest]\n",
    "\n",
    "\n",
    "# %%\n",
    "likert_replacer = {\n",
    "    \"Strongly disagree\": 1,\n",
    "    \"Somewhat disagree\": 2,\n",
    "    \"Neither disagree nor agree\": 3,\n",
    "    \"Somewhat agree\": 4,\n",
    "    \"Strongly agree\": 5,\n",
    "}\n",
    "\n",
    "likert_reverse_replacer = {\n",
    "    \"Strongly disagree\": 5,\n",
    "    \"Somewhat disagree\": 4,\n",
    "    \"Neither disagree nor agree\": 3,\n",
    "    \"Somewhat agree\": 2,\n",
    "    \"Strongly agree\": 1,\n",
    "}\n",
    "\n",
    "sub_df_2[likert_replace_vars].replace(likert_replacer, inplace=True)\n",
    "sub_df_2[likert_replace_vars] = sub_df_2[likert_replace_vars].astype(\"float\")\n",
    "\n",
    "sub_df_2[likert_reverse_replace_vars].replace(likert_reverse_replacer, inplace=True)\n",
    "sub_df_2[likert_reverse_replace_vars] = sub_df_2[likert_reverse_replace_vars].astype(\n",
    "    \"float\"\n",
    ")\n",
    "\n",
    "sub_df_2[\"partyid5_numeric\"] = sub_df_2.partyid5.cat.codes + 1\n",
    "# %%\n",
    "mode_impute_vars = ss_10_full + race\n",
    "sub_df_2[mode_impute_vars] = sub_df_2[mode_impute_vars].fillna(\n",
    "    sub_df_2[mode_impute_vars].mode().iloc[0]\n",
    ")\n",
    "\n",
    "sub_df_2[\"ss_6_past\"] = sub_df_2[ss_6_past].mean(axis=1)\n",
    "sub_df_2[\"ss_6_current\"] = sub_df_2[ss_6_current].mean(axis=1)\n",
    "\n",
    "sub_df_2[\"ss_past\"] = sub_df_2[ss_past].mean(axis=1)\n",
    "sub_df_2[\"ss_current\"] = sub_df_2[ss_current].mean(axis=1)\n",
    "\n",
    "flag_threshold = 3\n",
    "\n",
    "sub_df_2[\"race_view_flag\"] = np.where(\n",
    "    (sub_df_2[\"race_whiteadvantage\"] > flag_threshold)\n",
    "    | (sub_df_2[\"race_rich\"] > flag_threshold),\n",
    "    1,\n",
    "    0,\n",
    ")\n",
    "\n",
    "# %%\n",
    "# clean up some of the categoricals to be consistently coded\n",
    "sub_df_1.familycrimjust_ever.replace({0: \"No\", 1: \"Yes\"}, inplace=True)\n",
    "sub_df_1.familyuse_ever.replace({\" No\": \"No\"}, inplace=True)\n",
    "sub_df_1.personalcrimjust_ever.replace(\n",
    "    {\n",
    "        \"Yes, ever arrested or incarcerated\": \"Yes\",\n",
    "        \"No, never arrested or incarcerated\": \"No\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# impute any missing - confirm missing eliminated\n",
    "\n",
    "# # impute missing stigma scale score vals with median, impute missing personaluse_ever with mode, \"No\"\n",
    "\n",
    "# replace missing values of personaluse_ever with mode value of 'No'\n",
    "sub_df_1.personaluse_ever.fillna(\"No\", inplace=True)\n",
    "# print(sub_df_1.isnull().sum())\n",
    "\n",
    "sub_df_1.familyuse_ever.fillna(\"No\", inplace=True)\n",
    "# print(sub_df_1.isnull().sum())\n",
    "\n",
    "sub_df_1.personalcrimjust_ever.fillna(\"No\", inplace=True)\n",
    "# print(sub_df_1.isnull().sum())\n",
    "\n",
    "sub_df_1.familycrimjust_ever.fillna(\"No\", inplace=True)\n",
    "# print(sub_df_1.isnull().sum())\n",
    "\n",
    "\n",
    "# impute missing stigma scale score values as the median score\n",
    "# sub_df_1['stigma_scale_score'].fillna(sub_df_1.groupby('time-point')['stigma_scale_score'].transform('median'),inplace=True)\n",
    "sub_df_1[\"stigma_scale_score\"].fillna(\n",
    "    sub_df_1[\"stigma_scale_score\"].median(), inplace=True\n",
    ")\n",
    "sub_df_1[\"expanded_10item_stigma\"].fillna(\n",
    "    sub_df_1[\"expanded_10item_stigma\"].median(), inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "# %%\n",
    "# add df column with state 2 letter code\n",
    "# https://pythonfix.com/code/us-states-abbrev.py/\n",
    "# state name to two letter code dictionary\n",
    "us_state_to_abbrev = package.get_resource(\"state-abbreviations\").read_data()\n",
    "state_cd = sub_df_1.state.replace(us_state_to_abbrev)\n",
    "sub_df_1.insert(6, \"state_cd\", state_cd, True)\n",
    "\n",
    "# %%\n",
    "# Add jcoin information\n",
    "jcoin_json = package.get_resource(\"jcoin-states\").read_data()\n",
    "\n",
    "jcoin_df = (\n",
    "    pd.DataFrame(jcoin_json)\n",
    "    .assign(hub_types=lambda df: df[\"hub\"] + \"(\" + df[\"type\"] + \")\")\n",
    "    .groupby(\"states\")\n",
    "    # make a list of the name and type of hub/study and how many hubs are in that state\n",
    "    .agg({\"hub_types\": lambda s: \",\".join(s), \"hub\": \"count\"})\n",
    "    .reset_index()\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"states\": \"state_cd\",\n",
    "            \"hub\": \"jcoin_hub_count\",\n",
    "            \"hub_types\": \"jcoin_hub_types\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "jcoin_df[\"jcoin_flag\"] = 1\n",
    "\n",
    "# %%\n",
    "jcoin_df.head()\n",
    "\n",
    "# %%\n",
    "sub_df_1 = sub_df_1.merge(jcoin_df, on=\"state_cd\", how=\"left\")\n",
    "sub_df_1[\"jcoin_hub_types\"].fillna(\"not JCOIN\", inplace=True)\n",
    "sub_df_1[\"jcoin_hub_count\"].fillna(0, inplace=True)\n",
    "sub_df_1[\"jcoin_flag\"].fillna(0, inplace=True)\n",
    "sub_df_1[\"is_jcoin_hub\"] = np.where(\n",
    "    sub_df_1[\"jcoin_hub_types\"] == \"not JCOIN\", \"No\", \"Yes\"\n",
    ")\n",
    "\n",
    "\n",
    "# %%\n",
    "# join strata into dataset\n",
    "sub_df_1 = sub_df_1.merge(sub_df_2, on=\"caseid\", how=\"left\")\n",
    "\n",
    "# %%\n",
    "# o\tall: n/weighted n in genpop\n",
    "# o\tall:n/weighted n in as oversample\n",
    "# o\tall:n/weighted n in as oversample + gen pop(in oversampled state)\n",
    "# o\tper state: n/weighted n in as oversample\n",
    "# o\tmissingness; imputation procedures\n",
    "\n",
    "# %%\n",
    "pop_counts_by_sampletypexstate = (\n",
    "    sub_df_1.convert_dtypes()\n",
    "    .assign(jcoin_hub_count=lambda df: df.jcoin_hub_count.astype(str))\n",
    "    .groupby([\"state_cd\", \"p_over\"])[\"stigma_scale_score\"]\n",
    "    .count()\n",
    "    .unstack([\"p_over\"])\n",
    ")\n",
    "pop_counts_by_sampletypexstate[\"total\"] = pop_counts_by_sampletypexstate.sum(axis=1)\n",
    "\n",
    "# %%\n",
    "# merge jcoin info\n",
    "pop_counts_by_sampletypexstate = (\n",
    "    pop_counts_by_sampletypexstate.merge(jcoin_df, on=\"state_cd\", how=\"left\")\n",
    "    .sort_values(\"total\", ascending=False)\n",
    "    .assign(\n",
    "        jcoin_hub_count=lambda df: df.jcoin_hub_count.fillna(0).astype(int),\n",
    "        jcoin_flag=lambda df: df.jcoin_flag.fillna(0).astype(int),\n",
    "        jcoin_hub_types=lambda df: (\n",
    "            np.where(\n",
    "                df.jcoin_hub_types.isna() & df[\"AS oversample\"] > 0,\n",
    "                \"non JCOIN comparison\",\n",
    "                np.where(\n",
    "                    df.jcoin_hub_types.isna(), \"non JCOIN gen pop\", df.jcoin_hub_types\n",
    "                ),\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# %%\n",
    "states_with_oversample_df = pop_counts_by_sampletypexstate[\n",
    "    pop_counts_by_sampletypexstate[\"AS oversample\"] > 0\n",
    "]\n",
    "states_with_oversample_list = states_with_oversample_df[\"state_cd\"]\n",
    "\n",
    "\n",
    "sub_df_1_as_oversample_states = sub_df_1[\n",
    "    sub_df_1[\"state_cd\"].isin(states_with_oversample_list)\n",
    "]\n",
    "\n",
    "# %%\n",
    "# get caseids for survey respondents in oversampled states\n",
    "caseid_in_as_oversample_state = sub_df_1_as_oversample_states[\"caseid\"]\n",
    "\n",
    "# %%\n",
    "strata_df = package.get_resource(\"strata-and-psu\").to_pandas()\n",
    "strata_df.columns = strata_df.columns.str.lower()\n",
    "\n",
    "# get strata and cluster ids for survey respondents in oversampled states\n",
    "strata_df_in_as_oversample_state = strata_df[\n",
    "    strata_df[\"caseid\"].isin(caseid_in_as_oversample_state)\n",
    "]\n",
    "\n",
    "# collapse strata containing only 1 PSU\n",
    "onepsu = (\n",
    "    strata_df[[\"vstrat32\", \"vpsu32\"]]\n",
    "    .drop_duplicates()\n",
    "    .groupby(\"vstrat32\")\n",
    "    .count()\n",
    "    .squeeze()\n",
    "    .loc[lambda s: s == 1]\n",
    "    .index\n",
    ")\n",
    "strata_df[\"vstrat32_corrected\"] = strata_df[\"vstrat32\"].where(\n",
    "    cond=lambda s: ~s.isin(onepsu), other=-1\n",
    ")\n",
    "# rename PSUs so no duplicates\n",
    "strata_df[\"vpsu32_corrected\"] = strata_df.groupby(\n",
    "    [\"vstrat32_corrected\", \"vpsu32\"]\n",
    ").ngroup()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# collapse strata containing only 1 PSU\n",
    "onepsu_in_as_oversample_state = (\n",
    "    strata_df_in_as_oversample_state[[\"vstrat32\", \"vpsu32\"]]\n",
    "    .drop_duplicates()\n",
    "    .groupby(\"vstrat32\")\n",
    "    .count()\n",
    "    .squeeze()\n",
    "    .loc[lambda s: s == 1]\n",
    "    .index\n",
    ")\n",
    "strata_df_in_as_oversample_state[\"vstrat32_corrected\"] = (\n",
    "    strata_df_in_as_oversample_state[\"vstrat32\"].where(\n",
    "        cond=lambda s: ~s.isin(onepsu_in_as_oversample_state), other=-1\n",
    "    )\n",
    ")\n",
    "# rename PSUs so no duplicates\n",
    "strata_df_in_as_oversample_state[\"vpsu32_corrected\"] = (\n",
    "    strata_df_in_as_oversample_state.groupby([\"vstrat32_corrected\", \"vpsu32\"]).ngroup()\n",
    ")\n",
    "\n",
    "\n",
    "# %%\n",
    "# join strata into dataset\n",
    "\n",
    "fullsample_strata_df = strata_df.set_index(\"caseid\")[\n",
    "    [\"vstrat32_corrected\", \"vpsu32_corrected\"]\n",
    "].rename(\n",
    "    columns={\n",
    "        \"vstrat32_corrected\": \"strata_fullsample\",\n",
    "        \"vpsu32_corrected\": \"psu_fullsample\",\n",
    "    }\n",
    ")\n",
    "oversample_strata_df = strata_df.set_index(\"caseid\")[\n",
    "    [\"vstrat32_corrected\", \"vpsu32_corrected\"]\n",
    "].rename(\n",
    "    columns={\n",
    "        \"vstrat32_corrected\": \"strata_oversample\",\n",
    "        \"vpsu32_corrected\": \"psu_oversample\",\n",
    "    }\n",
    ")\n",
    "sub_df_1 = (\n",
    "    sub_df_1.set_index(\"caseid\")\n",
    "    .join(fullsample_strata_df)\n",
    "    .join(oversample_strata_df)\n",
    "    .assign(\n",
    "        isin_state_with_oversample=lambda df: df.index.isin(\n",
    "            caseid_in_as_oversample_state\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "sub_df_1.to_csv(\"data/processed/protocol2_wave1_analytic.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
